# Word Embedding Layer
This project uses a Word Embedding Layer to convert one-hot encoded text data into dense vector representations. The process is as follows:

# One-Hot Encoding
Each word in the input text is first converted into a one-hot encoded format. This ensures that each word is represented as a unique binary vector.

# Embedding Layer
The one-hot encoded vectors are then passed through an embedding layer, which maps each word to a lower-dimensional continuous vector space. This helps capture semantic relationships between words, enabling the model to better understand context and meaning.

The embedding layer reduces the high-dimensional one-hot vectors into dense vectors, significantly improving the efficiency and performance of the model.
